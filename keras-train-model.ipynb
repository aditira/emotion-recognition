{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model CNN - Emotion Recognition\n",
    "\n",
    "Content:\n",
    "- Build the CNN Model with Keras\n",
    "- Dataset Preparation and Addition of Output Layers\n",
    "- Callbacks in Keras\n",
    "- Training the Model\n",
    "- Visualization of Training Results\n",
    "- Use the model and add conclusion\n",
    "- Collaborate with ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network (CNN)** adalah salah satu jenis deep learning model yang paling umum digunakan untuk memproses gambar. CNN mampu memahami fitur kompleks dari gambar melalui proses pelatihan yang optimal. \n",
    "\n",
    "Secara visual, dapat kita lihat sebagai berikut:\n",
    "\n",
    "![Model CNN Architecture](./assets/images/backpropagation-algo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crete CNN model with Keras\n",
    "\n",
    "Pada praktek ini, kita melatih model CNN untuk mengerjakan tugas pengenalan emosi dari gambar wajah. Kita menggunakan package keras untuk membangun arsitektur CNN dan kita mulai dengan model sequential. CNN ini terdiri dari 3 blok Convolutional Layers (Conv2D + MaxPooling), diikuti oleh Flatten Layer, Dense Layer dan Dropout Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model sekuensial adalah jenis model yang membangun layer dalam urutan linear, di mana setiap layer memiliki tepat satu input tensor dan satu output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Membuat layer konvolusional:\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menambahkan layer konvolusional kedalam model. Layer ini memiliki 32 filter dengan ukuran (3, 3) dan menggunakan fungsi aktivasi ReLU (Rectified Linear Unit). Lapisan ini juga menentukan bentuk input yang akan disertakan dalam model, dalam kasus ini bentuknya adalah (48, 48, 1).\n",
    "\n",
    "Bentuk (48, 48, 1) merujuk kepada dimensi dari input yang akan diolah oleh model. Dalam konteks ini, itu berarti gambar dengan dimensi 48 x 48 piksel dan 1 merujuk kepada jumlah saluran warna atau channels yang diharapkan pada gambar.\n",
    "\n",
    "Ketika:\n",
    "\n",
    "1 channel: Gambar tersebut adalah grayscale (hitam dan putih)\n",
    "3 channels: Gambar tersebut adalah berwarna, umumnya RGB (Red, Green, Blue)\n",
    "Jadi, dalam kasus ini, model Anda membutuhkan gambar dengan dimensi 48x48 piksel dalam format grayscale sebagai input.\n",
    "\n",
    "Lapisan konvolusi diikuti oleh layer pooling (Max Pooling) dengan ukuran pool (2,2). Layer MaxPooling digunakan untuk mengurangi dimensi spasial dari output volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), activation='relu', ))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menambahkan dua layer konvolusional tambahan dan setiap layer diikuti oleh layer MaxPooling. Filter meningkat dua kali lipat setiap kali (64 dan 128), hal ini umum dalam arsitektur CNN karena ini memungkinkan model untuk belajar fitur yang lebih kompleks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Flatten digunakan untuk mengubah input menjadi vektor 1D (mengubah semua data menjadi satu dimensi linear panjang). Ini biasanya digunakan sebelum membentuk layer Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menambahkan layer dense dengan 256 neuron dan fungsi aktivasi ReLU. Layer Dense ini secara penuh (fully) terhubung, artinya semua neuron di layer sebelumnya terhubung dengan semua neuron di layer ini.\n",
    "\n",
    "Layer Dropout juga ditambahkan setelah layer Dense tersebut, yang secara acak menyetel sebagian input ke 0 saat waktu training, yang membantu untuk mencegah overfitting. Dalam hal ini, 50% (0.5) dari input akan di-dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = 'dataset/web'\n",
    "sub_dirs = [name for name in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, name))]\n",
    "num_class = len(sub_dirs)\n",
    "print(f\"Number of classes: {num_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membaca direktori dataset dan menghitung jumlah sub-direktori (kelas) dalam dataset tersebut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_class, activation='softmax')) # jumlah kelas emosi yang akan di prediksi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menambahkan lapisan Dense ke model, jumlah neuron sama dengan jumlah kelas dalam dataset (num_class). Fungsi aktivasi yang digunakan adalah softmax, yang umum digunakan untuk lapisan output dalam masalah klasifikasi multi-kelas. Output dari softmax akan memberikan probabilitas untuk setiap kelas dan jumlah dari semua probabilitas akan menjadi 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengkompilasi model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengkompilasi model dengan loss function, optimizer, dan metrik.\n",
    "\n",
    "- `categorical_crossentropy` adalah loss function yang umum digunakan untuk masalah klasifikasi multi-kelas.\n",
    "- `adam` adalah jenis optimizer. Adam adalah algoritma optimasi yang dapat digunakan sebagai pengganti prosedur stochastic gradient descent klasik untuk memperbarui bobot jaringan secara iteratif berdasarkan data training.\n",
    "- `accuracy` adalah metrik yang akan digunakan untuk mengevaluasi performa model.\n",
    "\n",
    "Jadi, pembuatan model CNN sudah selesai, di mana layer output ditambahkan dan model dikompilasi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "Kode ini digunakan untuk memuat dataset dari direktori yang ditentukan. Kami telah membagi dataset menjadi training dan test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Natural Human Face Images for Emotion Recognition - Kaggle](https://www.kaggle.com/datasets/sudarshanvaidya/random-images-for-face-emotion-recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Data augmentation adalah teknik yang digunakan untuk meningkatkan jumlah data dengan menambahkan versi modifikasi dari gambar ke dalam pool. Teknik ini memungkinkan model mempelajari data melalui berbagai titik pandang dan mengurangi overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Membuat generator gambar dengan augmentasi data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=15,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) # test data tetap hanya di rescale\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(dataset_path, \n",
    "                                                 target_size=(48, 48), \n",
    "                                                 batch_size=32, \n",
    "                                                 color_mode='grayscale', \n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(dataset_path, \n",
    "                                            target_size=(48, 48), \n",
    "                                            batch_size=32, \n",
    "                                            color_mode='grayscale', \n",
    "                                            class_mode='categorical')\n",
    "\n",
    "print(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks adalah objek khusus yang bisa Anda sisipkan ke dalam proses training untuk menyesuaikan proses tersebut atau melakukan tindakan tertentu setiap kali kondisi tertentu terpenuhi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "earlystop = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback `EarlyStopping` digunakan untuk menghentikan training ketika sebuah metrik tertentu berhenti meningkat. Dalam hal ini, model akan berhenti training jika tidak ada peningkatan dalam metrik yang diperiksa (defaultnya adalah loss validation) setelah 5 epoch (dengan asumsi `patience=5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model/keras/best_model.h5',\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback `ModelCheckpoint`` digunakan untuk menyimpan model setelah setiap epoch.\n",
    "\n",
    "- `model/cnn/best_model.h5` adalah file tempat model dengan performa terbaik akan disimpan.\n",
    "- `monitor='val_loss'` berarti callback ini akan memantau loss validation dari model.\n",
    "- `verbose=1` berarti callback ini akan mencetak pesan saat model disimpan.\n",
    "- `save_best_only=True` berarti hanya model dengan loss validation terkecil yang akan disimpan. Jika False, maka model setelah setiap epoch akan disimpan.\n",
    "- `mode='auto'` berarti arah peningkatan atau penurunan metrik yang dipantau (dalam hal ini val_loss) akan otomatis dideteksi dan digunakan untuk memutuskan kapan harus menyimpan model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [reduce_lr, earlystop, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat list yang berisi kedua callback ini. Anda nantinya akan memberikan list ini sebagai argumen untuk parameter callbacks ketika memanggil method fit() untuk melatih model Anda. Callbacks dalam list ini akan dipanggil dalam urutan yang sama seperti yang ditentukan dalam list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Akhirnya, kita melatih model dengan dataset yang disiapkan menggunakan callback seperti `EarlyStopping` dan `ModelCheckpoint` untuk menghentikan pelatihan jika model tidak lagi memperbaiki hasil setelah beberapa epoch dan untuk menyimpan model dengan hasil terbaik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan training model dengan data yang sudah di augmentasi\n",
    "history = model.fit(training_set, \n",
    "        #   steps_per_epoch=training_set.samples//32, \n",
    "          validation_data=test_set, \n",
    "        #   validation_steps=test_set.samples//32, \n",
    "          epochs=100, # Bisa menambahkan lebih banyak epoch karena kita menggunakan callback\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips:** ada beberapa hal yang bisa dilakukan untuk meningkatkan kinerja model:\n",
    "\n",
    "- **Tambahkan lebih banyak data**: Jika Anda memiliki lebih banyak data, model dapat belajar pola yang lebih bervariasi, yang dapat meningkatkan akurasi.\n",
    "- **Data augmentation**: Jika Anda tidak memiliki cukup data, Anda bisa mencoba teknik augmentasi data seperti flipping, rotating, zooming, dll.\n",
    "- **Tambahkan lebih banyak layer atau ubah parameter**: Anda bisa mencoba menambahkan lebih banyak layer konvolusi atau dense, atau mengubah jumlah neuron dalam layer tersebut. Anda juga bisa menambahkan atau mengubah dropout rate.\n",
    "- **Epoch**: Anda bisa mencoba meningkatkan jumlah epoch. Dengan melakukan lebih banyak iterasi, model mungkin dapat belajar lebih baik. Namun perhatikan untuk jangan sampai overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize change in loss and model accuracy\n",
    "\n",
    "Memvisualisasikan perubahan loss (kerugian) dan akurasi model selama proses training dan validasi dengan menggunakan `Matplotlib`, library visualisasi data di Python. `history` adalah objek yang dikembalikan oleh method `fit()` saat melatih model dan berisi catatan loss dan metrik lainnya setiap kali epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend([\"Train\", \"Loss\"], loc=\"upper left\")\n",
    "plt.title('Loss')\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend([\"Train\", \"Loss\"], loc=\"upper left\")\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jadi, kode ini pada dasarnya membuat dua plot: satu untuk loss dan satu untuk akurasi. Untuk setiap plot, nilai training dan validasi ditampilkan. Ini adalah cara yang baik untuk memvisualisasikan proses training dan untuk melihat apakah model overfit (jika loss training menurun tetapi loss validasi mulai meningkat) atau underfit (jika loss training dan validasi masih tinggi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from IPython.display import Image, display\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "model = load_model('model/keras/best_model.h5')\n",
    "\n",
    "img_path = 'dataset/web/happy/image-2.jpg'\n",
    "# Load image\n",
    "img = load_img(img_path, target_size=(48, 48), color_mode='grayscale')\n",
    "\n",
    "# Convert image to array and expand dimension\n",
    "img_array = img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Standardize (Rescale pixel values)\n",
    "test_data = img_array / 255.0\n",
    "\n",
    "# Make prediction\n",
    "preds = model.predict(test_data)\n",
    "display(Image(img_path))\n",
    "print(preds[0])\n",
    "slov_res ={}\n",
    "for i in training_set.class_indices:\n",
    "    slov_res[i] = preds[0][training_set.class_indices[i]]\n",
    "    \n",
    "\n",
    "sorted_keys = sorted(slov_res, key=slov_res.get, reverse = True)\n",
    "sorted_dict = {}\n",
    "for w in sorted_keys:\n",
    "    sorted_dict[w] = slov_res[w]\n",
    "    \n",
    "for i in sorted_dict:\n",
    "    print(str(i) + ': ' + str(sorted_dict[i]*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kesimpulan:\n",
      "Gambar tersebut sebagian besar menunjukkan emosi happiness dengan tingkat keyakinan 99.95067119598389%.\n",
      "Selain itu, model juga merasakan emosi surprise dengan tingkat keyakinan 0.0251894467510283%.\n"
     ]
    }
   ],
   "source": [
    "top_emotion = list(sorted_dict.keys())[0]\n",
    "top_emotion_percentage = sorted_dict[top_emotion]*100\n",
    "second_emotion = list(sorted_dict.keys())[1]\n",
    "second_emotion_percentage = sorted_dict[second_emotion]*100\n",
    "\n",
    "print(f\"\\nKesimpulan:\")\n",
    "print(f\"Gambar tersebut sebagian besar menunjukkan emosi {top_emotion} dengan tingkat keyakinan {top_emotion_percentage}%.\")\n",
    "print(f\"Selain itu, model juga merasakan emosi {second_emotion} dengan tingkat keyakinan {second_emotion_percentage}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborate with ChatGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's great to hear that! Being happy is a wonderful feeling. Is there anything specific you would like assistance with related to happiness?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Generate a response\n",
    "response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"The person seems to be \" + top_emotion}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get the response\n",
    "output = response['choices'][0]['message']['content']\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
